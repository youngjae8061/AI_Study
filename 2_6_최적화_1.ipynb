{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2-6 최적화_1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youngjae8061/AI_Study/blob/main/2_6_%EC%B5%9C%EC%A0%81%ED%99%94_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tchy4SprcBY"
      },
      "source": [
        "- Multi-variable Gradient descent (다변수 최적화)\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-kY8OEor__Y"
      },
      "source": [
        "Review. 단일변수 Gradient descent\r\n",
        "\r\n",
        "  $x \\leftarrow x - {{f'(x)}\\over{f''(x)}}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPVJbun5sBgz"
      },
      "source": [
        "다변수에서 Gradient descent?? \r\n",
        "\r\n",
        "반드시 알아야해요! 딥러닝의 학.습.대.상이 다.변.수.함.수!!!!!!!!!!!!!!!!!!!!!!!!\r\n",
        "\r\n",
        "$X = \\begin{bmatrix} \r\n",
        "  x_1 \\\\\r\n",
        "  x_2 \\\\\r\n",
        "  \\vdots \\\\\r\n",
        "\\end{bmatrix}$\r\n",
        "\r\n",
        "$X \\leftarrow X - \\alpha \\sum{{F'(X)}}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccDX5KIgraBl"
      },
      "source": [
        "# 2차원 stochastic gradient descent\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "%matplotlib inline\r\n",
        "\r\n",
        "x1 = np.linspace(-3,3)\r\n",
        "x2 = np.linspace(-3,3)\r\n",
        "X1,X2 = np.meshgrid(x1,x2)\r\n",
        "\r\n",
        "def test_func(X):\r\n",
        "  return X**2 # (x1, x2)가 (0, 0)일때 최소값\r\n",
        "\r\n",
        "def grad_test(X):\r\n",
        "  return 2*X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8DIBWOrm-Qw"
      },
      "source": [
        "X = [-1.78,-1.85]\r\n",
        "x1,x2 = X\r\n",
        "Z = test_func(X1,X2)\r\n",
        "plt.contour(X1,X2,Z)\r\n",
        "plt.plot(x1,x2,'ro')\r\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ynyy81JqDRk"
      },
      "source": [
        "ans_buff = []\r\n",
        "for iter in range(10):\r\n",
        "  ans_buff.append(X)\r\n",
        "  X = X- 0.2*np.mean(grad_test(X[0],X[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg3v-o0Wtceg"
      },
      "source": [
        "ans_buff = np.array(ans_buff)\r\n",
        "plt.contour(X1,X2,Z)\r\n",
        "plt.plot(ans_buff[:,0],ans_buff[:,1],'ro-') # x1좌표 10개 ans_buff[:,0],   x2좌표 10개 ans_buff[:,1]   빨간점을 직선으로\r\n",
        "plt.colorbar()\r\n",
        "#print(ans)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8dueDF1ttxv"
      },
      "source": [
        "print(ans_buff[-1]) # 최종 좌표"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIbtml0ZT_IS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}